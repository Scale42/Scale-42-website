<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Scale42 thought piece: DeepSeek and accelerated AI | Scale42*</title>
    <meta name="description" content="Exploring how accelerated computing clusters enable the rapid training of frontier AI models like DeepSeek." />
    <link rel="icon" type="image/png" href="../assets/images/scale42-logo-small.png" />
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lexend:wght@400;500;600;700;800&family=Montserrat:wght@400;500;600;700&display=swap" />
    <link rel="stylesheet" href="../assets/css/styles.css" />
  </head>
  <body>
    <header style="background: #ffffff; padding: 48px 0; text-align: center;">
      <div class="container">
        <img src="../assets/images/scale42-logo.png" alt="Scale42*" style="max-width: 200px; margin: 0 auto 1rem auto; filter: brightness(0); display: block;" />
        <h1 style="color: #333; font-size: 2rem; margin: 0 auto; max-width: 800px;">Scale42 Thought Piece - DeepSeek</h1>
      </div>
    </header>

    <main>
      <section class="posts" style="padding: 96px 0;">
        <div class="container">
          <article style="max-width: 800px; margin: 0 auto;">
            <div style="margin-bottom: 2rem;">
              <span style="background: var(--accent); color: #02060f; padding: 4px 8px; border-radius: 12px; font-weight: 600; font-size: 12px;">Perspective</span>
              <time style="margin-left: 1rem; color: #666666;">29 January 2024</time>
              <span style="margin-left: 1rem; color: #666666;">• By Jamie Stewart</span>
            </div>
            
            <div style="margin-bottom: 2rem; font-style: italic; color: #666;">
              <p><strong>Updated:</strong> 1 May 2024</p>
              <p><strong>Authors:</strong> Will Tasney, Thomas Beaton, Jamie Stewart</p>
            </div>
            
            <img src="../assets/images/posts/089945_74dd324de2e544abb91900c477c86fd0~mv2.jpeg" alt="Conceptual illustration of accelerated AI compute" style="width: 100%; height: 400px; object-fit: cover; border-radius: 12px; margin-bottom: 2rem;" />
            
            <div style="line-height: 1.7; color: #333;">
              <h2 style="color: #333; margin-top: 2rem;">DeepSeek Disruption - Dispersion of AI Computing Demand</h2>
              
              <h3 style="color: #333; margin-top: 2rem;">Introduction</h3>
              
              <p>DeepSeek's innovative approach to AI training has disrupted the market, however, the stated efficiency and cost claims have likely been overstated for two reasons. Firstly to avoid suspicion on breaking US Semi-conductor sanctions and secondly, to cause maximum damage when launched as a geopolitical grenade to challenge US AI leadership. We examine the technology, its implications and winners and losers and conclude that it aligns well with our long-term outlook for the AI sector set out in our pitch-deck.</p>
              
              <h3 style="color: #333; margin-top: 2rem;">Summary</h3>
              
              <ul style="margin: 1rem 0; padding-left: 2rem;">
                <li>Efficiency and cost gains made by DeepSeek have likely been overstated</li>
                <li>The achievement by DeepSeek and resulting gain in efficiency was through using <strong>PTX</strong> as opposed to NVIDIA's CUDA as the programming language</li>
                <li>This is not practical for most AI companies to achieve</li>
                <li>Some reports indicate that DeepSeek have a lot more GPU resources than they have indicated, up to <strong>60,000 GPUs</strong> as opposed to the 2,048 GPUs stated by DeepSeek</li>
                <li>This model may have two years to develop sacrificing speed for lower cost</li>
                <li>More open-source AI IP piece</li>
                <li>Scale42 believes that more competition and open-source AI software will accelerate the requirement for mid-size AI training facilities that we specialise in</li>
              </ul>
              
              <h3 style="color: #333; margin-top: 2rem;">What is DeepSeek?</h3>
              
              <p>DeepSeek is an open source AI Mixture-of-Experts (MoE) language model, with <strong>671 billion parameters</strong> stated to have used a specialized cluster of 2,048 Nvidia H800 GPUs (an H100 GPU variant developed by NVIDIA to not violate US sanctions).</p>
              
              <p>It is claimed that in just two months with a small GPU stack, they trained an AI that is comparable or better than similar models developed by industry leaders such as OpenAI and Meta.</p>
              
              <p>It is estimated that the computing intensity needed to reach a product of this level was <strong>10x higher than stated</strong>, making DeepSeek 10x more efficient than competitors.</p>
              
              <p>DeepSeek has innovated by using <strong>PTX (Parallel Thread Execution)</strong> programming instead of Nvidia's standard CUDA language used almost exclusively by the vast majority of the AI industry.</p>
              
              <p>This is significant because NVIDIA and competing hardware providers (such as AMD) have produced GPUs with similar hardware specifications. Despite this NVIDIA has accumulated a majority market share for AI GPU sales, principally achieved due to NVIDIA's proprietary CUDA programming language which has become the industry standard for AI developers.</p>
              
              <p>DeepSeek is a spinout from a Chinese quantitative hedge fund 'High-Flyer' which in turn is reported to have purchased an estimated <strong>10,000 - 60,000 NVIDIA GPUs</strong>. The breakthrough in programming optimization has helped DeepSeek outperform conventional methods.</p>
              
              <h3 style="color: #333; margin-top: 2rem;">DeepSeek's Atomic Impact</h3>
              
              <p>The low cost, short time and minimal resources stated as being required to train DeepSeek has underpinned the market's reaction. Stated as costing just <strong>$6m</strong> to build something competitors have sunk billions into, and then releasing the IP for free, it undermined the sunk investments and balance sheets of IP holders like OpenAI.</p>
              
              <p>However, the true cost of DeepSeek is not known and the $6m figure could be an exercise in creative accounting. Even taking DeepSeek at face value, 10,000 Nvidia A100/H800s costs an estimated <strong>$300m</strong>, 50,000 though would cost <strong>$1.5bn</strong>.</p>
              
              <p>DeepSeek's announcement and release just days after the US announced its $500bn Stargate programme has led some to believe that it was co-opted by the CCP to undermine US confidence in the AI race.</p>
              
              <h3 style="color: #333; margin-top: 2rem;">Market Impact Analysis</h3>
              
              <p>Ultimately the biggest losers here are those companies that have pursued a model that seeks to rent their AI models. At the front of this is OpenAI's ChatGPT, Meta's Llama, Google's Gemini, Anthropic/Amazon's Claude among others.</p>
              
              <p>If AI IP was in the DeepSeek blast zone, it was chipmakers that found themselves nearby suffering from significant radiation with shares down over 10%.</p>
              
              <blockquote style="border-left: 4px solid var(--accent); padding-left: 1rem; margin: 2rem 0; font-style: italic; color: #555;">
                "If we acknowledge that DeepSeek may have reduced costs of achieving equivalent model performance by, say, 10x, we also note that current model cost trajectories are increasing by about that much every year anyway... we NEED innovations like this... as semi analysts we are firm believers in the Jevons paradox (i.e. that efficiency gains generate a net increase in demand)"
                <footer style="margin-top: 0.5rem; font-size: 0.9em;">— Stacy Ragson, Semiconductor Analyst at Bernstein</footer>
              </blockquote>
              
              <h3 style="color: #333; margin-top: 2rem;">Implications for Data Centre Infrastructure</h3>
              
              <p>Consider the sell-off in electricity companies near data centre hotspots. The market may be saying the electricity demand won't be where we thought, that is, near a few huge proprietary data centres run by giant US tech companies. Instead, AI will be run on smaller data centres all over the place.</p>
              
              <p>Scale42's own experience has been that capital market participants have been searching for ever greater MW sites for placing data centres set to consume ever greater amounts of energy for large singular computing clusters. DeepSeek has put a question mark over the necessity of sites requiring hundreds of MW that have diseconomies of scale.</p>
              
              <p>Our sites are located first and foremost for access to <strong>cheap, clean energy</strong> and will be brought to market to meet real world demand.</p>
              
              <h3 style="color: #333; margin-top: 2rem;">DeepSeek Supports Scale42's Long Term View</h3>
              
              <p>In Scale42's pitch, we set out 5 areas where we predicted industry disruption. DeepSeek fits into several of these themes:</p>
              
              <ul style="margin: 1rem 0; padding-left: 2rem;">
                <li><strong>Nvidia hardware → Competing hardware providers:</strong> DeepSeek abandoned CUDA for PTX programming, potentially opening doors for better performing chips</li>
                <li><strong>Generic LLM & AI Tools → Optimised Enterprise Specific AI Tools:</strong> DeepSeek catalyzes open source tools built without rent-seeking from established leaders</li>
                <li><strong>Few Cutting Edge Providers → Proliferation of Bespoke Providers:</strong> DeepSeek exemplifies market widening with new competitors building on its foundations</li>
              </ul>
              
              <h3 style="color: #333; margin-top: 2rem;">Scale42's Strategic Position</h3>
              
              <p>Our conclusions remain unchanged:</p>
              
              <ul style="margin: 1rem 0; padding-left: 2rem;">
                <li>AI will entwine every level of the global economy as enterprises use their data to build custom AI tools</li>
                <li>Enterprise AI will require a deeper market for <strong>mid-sized AI infrastructure</strong></li>
                <li>Technology less centralised on NVIDIA</li>
                <li>More open-source tools lower the cost of AI tech and increase end demand for infrastructure</li>
              </ul>
              
              <p><strong>Scale42's approach to next phase of AI innovation:</strong></p>
              
              <ul style="margin: 1rem 0; padding-left: 2rem;">
                <li>Mid-scale training assets</li>
                <li>Cost leader</li>
                <li>Chip-agnostic</li>
              </ul>
              
              <p>The DeepSeek disruption validates our thesis that the future of AI infrastructure lies not in massive centralized facilities, but in distributed, efficient, and adaptable mid-scale data centres positioned at sources of renewable energy.</p>
              
              <p style="margin-top: 2rem;"><a href="../index.html#posts" style="color: var(--accent); text-decoration: none; font-weight: 600;">← Back to Scale42 Home</a></p>
            </div>
          </article>
        </div>
      </section>
    </main>

    <footer class="footer">
      <div class="container footer__content">
        <span>&copy; <span id="year">2024</span> Scale42*. All rights reserved.</span>
        <a href="../index.html#top" class="footer__link">Back to top</a>
      </div>
    </footer>
  </body>
</html>
